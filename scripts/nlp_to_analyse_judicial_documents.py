# -*- coding: utf-8 -*-
"""NLP to Analyse Judicial Documents

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VNDnV0NnujBnXxa04Y9bQCVWQemI_zpE

# A. **Importing Libraries**

### a. Importing Libraries for Modeling
"""

# from tika import parser
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn
import pandas as pd

"""### b. Syncing the Drive to Import Data"""

from google.colab import drive
drive.mount('/content/drive')

"""### c. Importing Libraries for Features"""

import nltk
from nltk.corpus import stopwords
#stopwords.words("english")
import re
porter = nltk.PorterStemmer()
WNLemma = nltk.WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

"""# B. **Extraction of Features (IPCs and Arm Acts)**

### a. From No Bail Files
"""

#ls = ("u/s" , "u\s" , "us." , "u.s")
#for i in ls:
#    print("[1]" , f.split("ipc")[0].split(i)[-1])
file_text_nobail = pd.DataFrame({

    "IPC" : [] ,
    "ARMS ACT" : [],
    "CATEGORY" : []

}
)

fileloc = "C:/Users/kusha/Desktop/Case File/No Bail/"
c = 0    #for no bail
for i in range(1,171) :   #changed
    fileloc = "C:/Users/kusha/Desktop/Case File/No Bail/"
    filename = i
    file = fileloc + str(filename) + ".pdf"
    raw = parser.from_file(file)
    text = (raw['content'])
    t1 = text.replace(u'\xad',"").lower().split()

    f = " ".join(t1)
    #"indian penal code" in f
    k = f.replace("indian penal code" , "ipc")
    k = k.replace("i.p.c" , "ipc")
    k = k.replace(" r/w. section" , "rws")
    k = k.replace(" r/w section", "rws")
    k = k.replace(" r\w. section" , "rws")
    k = k.replace(" r\w section", "rws")
    k = k.replace(" r/w" , "rws")
    k = k.replace(" r\w" , "rws")
    k = k.replace("read with section" , "rws")


    ls = ["u/s" , "u\s" , "us." , "u.s","section","under sec","sec"] #,"and"
    k1 = k
### LOOP FOR CHANGING TO UNDSEC
    for i in ls :
        if "arms act" in k:
            k1 = k1.split("arms act")[0].replace(i , "undsec")
        else:
            k1 = k1.split("ipc")[0].replace(i , "undsec")

### KEEPING IPC IN DATASET
#    rms = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("undsec")[-1])
#    ipc_list = re.findall(r'\d{1,3}?[a-z]{1,2}|\d{1,3}', rms)
### KEEPING ARMS ACT
    if "arms act" in k:
        #aa_text = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("undsec")[-1])
        aa_text = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("ipc")[-1].split("undsec")[-1])
        aa_list = re.findall(r'\d{1,3}?[a-z]{1,2}|\d{1,3}', aa_text)
        ipc_text = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("ipc")[0].split("undsec")[-1])
        ipc_list = re.findall(r'\d{1,3}', ipc_text)#?[a-z]{1,2}|\d{1,3}
    else:
        ipc_text = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("undsec")[-1])
        ipc_list = re.findall(r'\d{1,3}', ipc_text)#?[a-z]{1,2}|\d{1,3}
        aa_list = "NA"


    temp_dict = {"IPC": ",".join(ipc_list),"ARMS ACT" : ",".join(aa_list), "CATEGORY" : c}
    file_text_nobail = file_text_nobail.append(temp_dict, ignore_index=True, sort=False)

file_text_nobail.IPC[10]

"""### b. From Bail Files"""

file_text_bail = pd.DataFrame({

    "IPC" : [] ,
    "ARMS ACT" : [],
    "CATEGORY" : []

}
)


c = 1    #for bail
for i in range(1,345) :
    fileloc = "C:/Users/kusha/Desktop/Case File/Bail/"
    filename = i
    file = fileloc + str(filename) + ".pdf"
    raw = parser.from_file(file)
    text = (raw['content'])
    t1 = text.replace(u'\xad',"").lower().split()

    f = " ".join(t1)
    f = f.split("following order")[0]
    #"indian penal code" in f
    k = f.replace("indian penal code" , "ipc")
    k = k.replace("i.p.c" , "ipc")
    k = k.replace(" r/w. section" , "rws")
    k = k.replace(" r/w section", "rws")
    k = k.replace(" r\w. section" , "rws")
    k = k.replace(" r\w section", "rws")
    k = k.replace(" r/w" , "rws")
    k = k.replace(" r\w" , "rws")
    k = k.replace("read with section" , "rws")


    ls = ["u/s" , "u\s" , "us." , "u.s","section","under sec","sec","under se"]
    k1 = k
### LOOP FOR CHANGING TO UNDSEC
    for i in ls :
        if "arms act" in k:
            k1 = k1.split("arms act")[0].replace(i , "undsec")
        else:
            k1 = k1.split("ipc")[0].replace(i , "undsec")

### KEEPING IPC IN DATASET
#    rms = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("undsec")[-1])
#    ipc_list = re.findall(r'\d{1,3}?[a-z]{1,2}|\d{1,3}', rms)
### KEEPING ARMS ACT
    if "arms act" in k:
        #aa_text = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("undsec")[-1])
        #aa_text = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("ipc")[-1])
        aa_text = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("ipc")[-1].split("undsec")[-1])
        aa_list = re.findall(r'\d{1,3}?[a-z]{1,2}|\d{1,3}', aa_text)
        ipc_text = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("ipc")[0].split("undsec")[-1])
        ipc_list = re.findall(r'\d{1,3}', ipc_text)#?[a-z]{1,2}|\d{1,3}
    else:
        if "ipc" in k:
            ipc_text = re.sub(r"[^a-zA-Z0-9 ]", "", k1.split("undsec")[-1])
            ipc_list = re.findall(r'\d{1,3}', ipc_text)#?[a-z]{1,2}|\d{1,3}
            aa_list = ["NA"]
        else:
            aa_list = ["NA"]
            ipc_list = ["NA"]


    temp_dict = {"IPC": ",".join(ipc_list),"ARMS ACT" : ",".join(aa_list), "CATEGORY" : c}
    file_text_bail = file_text_bail.append(temp_dict, ignore_index=True, sort=False)

file_text_bail.IPC[242]

"""### c. Extracting set of all unique IPC"""

uni_ipc = (",".join(file_text_bail.IPC[0:343])+","+(",".join(file_text_nobail.IPC[0:169]))).split(",")
### joining text of ipc for each case from bail and no bail using comma

u = uni_ipc
uni_ipc = []
for i in u :
   if len(i)>1:
       uni_ipc.append(i)

uni_ipc = set(uni_ipc)

len(uni_ipc)

"""### d. Transforming the strings into desired input format for modeling

#### i. From Bail
"""

# Transforming the strings into desired input format for modeling (for Bail)

df_bail = pd.DataFrame(columns = uni_ipc) # Bail dataframe in 0s and 1s form

lr_bail = []

for j in range(0,344):
    for i in uni_ipc :
        if "," in file_text_bail.IPC[j] :
            if file_text_bail.IPC[j].startswith(i+",") or file_text_bail.IPC[j].endswith(","+i) or ((","+i+",") in file_text_bail.IPC[j]):
                lr_bail.append(1)
            else:
                lr_bail.append(0)
        else :
            if i == file_text_bail.IPC[j] :
                lr_bail.append(1)
            else :
                lr_bail.append(0)
    df_bail.loc[j] = lr_bail
    lr_bail = []

"""#### ii. From No Bail"""

# Transforming the strings into desired input format for modeling (for No Bail)

df_nobail = pd.DataFrame(columns = uni_ipc) # No Bail dataframe in 0s and 1s form

lr_nobail = []

for j in range(0,170):
    for i in uni_ipc :
#        if i in file_text_nobail.IPC[j] :
#            lr_nobail.append(1)
#        else :
#            lr_nobail.append(0)
        if "," in file_text_nobail.IPC[j] :
            if file_text_nobail.IPC[j].startswith(i+",") or file_text_nobail.IPC[j].endswith(","+i) or ((","+i+",") in file_text_nobail.IPC[j]):
                lr_nobail.append(1)
            else:
                lr_nobail.append(0)
        else :
            if i == file_text_nobail.IPC[j] :
                lr_nobail.append(1)
            else :
                lr_nobail.append(0)
    df_nobail.loc[j] = lr_nobail
    lr_nobail = []

# Changing Column Name NA to Others (acts),

df_nobail.rename(columns = {"NA" : "OTHER"} , inplace = True)

df_bail.rename(columns = {"NA" : "OTHER"} , inplace = True)

# Creating the Response Variable (y),
df_nobail["RESULT"] = 0

df_bail["RESULT"] = 1

# Removing wrongly extrated IPCs

ls_rem = ["018","010"] ## Removed
df_bail = df_bail.drop(ls_rem , axis = 1)

ls_rem = ["010","018"] ## Removed
df_nobail = df_nobail.drop(ls_rem , axis = 1)

# Getting the whole data frame [x,y]
df_ipc_zo = pd.concat([df_nobail, df_bail], axis=0)

#df_ipc_zo

# Renaming the index after concating bail and no bail,
df_ipc_zo.index = range(0 , len(df_ipc_zo))

# Dividing data into explanatory(X) and response variable(y)
X = df_ipc_zo.iloc[:, 0:-1]
y = df_ipc_zo.iloc[:, -1]

#X = X.drop(X.columns[X.sum(axis = 0) < 2] , axis = 1) # We are removing all ipc coming only once
## Accuracy worsen

"""### e. Saving the Data in a .csv file"""

df_ipc_zo.to_csv("Final Data", header = True, index = False)

"""# C. **Importing the Cleaned Data from .csv files**"""

# from google.colab import drive
# drive.mount('/content/drive')
df = pd.read_csv("/content/drive/MyDrive/Final Data")
df
df_bail = df.loc[df.index>=170]
df_nobail = df.loc[df.index<170]
X = df.iloc[:, 0:-1]
y = df.iloc[:, -1]

"""# D. **Exploratory Data Analysis**

### a. Plotting All Bail Files
"""

bail_ = np.sum(df_bail.iloc[:,0:-1] , axis = 0)

bail_ = bail_[bail_>10]

list1 = bail_[bail_ != 0].index
# creating the Y Coordinate data as a python list
list2 = bail_[bail_ != 0]/344
# plotting the bar graph
plt.barh(list1, list2)
# displaying the bar graph
plt.show()

"""### b. Plotting All No Bail Files"""

nbail_ = np.sum(df_nobail.iloc[:,0:-1] , axis = 0)

nbail_ = nbail_[nbail_>5]

list1 = nbail_[nbail_ != 0].index
# creating the Y Coordinate data as a python list
list2 = nbail_[nbail_ != 0]/170
# plotting the bar graph
plt.barh(list1, list2)
# displaying the bar graph
plt.show()

X_train.iloc[X_train.index<170].shape

"""# E. Classifiers for Unbalanced Data

### a. Bernoulli Naive Bayes
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

"""#### i. Fitting the Bernoulli Naive Bayes"""

#Training the Naive Bayes model on the training set

from sklearn.naive_bayes import BernoulliNB
classifier = BernoulliNB()
classifier.fit(X_train, y_train)

#Let’s predict the test results
y_pred  =  classifier.predict(X_test)

"""#### ii. Result of Bernoulli Naive Bayes"""

from sklearn.metrics import confusion_matrix,f1_score

# Confusion Matrix
cm1 = confusion_matrix(y_test,y_pred)
print('Confusion Matrix : \n', cm1)

# Getting sum of all cases (test)
total1=sum(sum(cm1))

# From confusion matrix calculate different confusin matrix metrics

accuracy1=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Sensitivity : ', sensitivity1 )

specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

FScore= f1_score(y_test, y_pred)
print('F-Score : ', FScore)

# conf_matrix graph
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(cm1, cmap=plt.cm.Blues, alpha=0.7)
for i in range(cm1.shape[0]):
    for j in range(cm1.shape[1]):
        ax.text(x=j, y=i,s=cm1[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

"""#### iii. Plot for Training Bail Files in Frequency Proportion"""

tb = X_train[X_train.index>74]
tb = np.sum(tb, axis = 0)
tb = tb[tb>10] ## Frequency coming greater than 10
list1 = tb[tb != 0].index
# creating the Y Coordinate data as a python list
list2 = tb[tb != 0]
# plotting the bar graph
plt.barh(list1, list2/271)
# displaying the bar graph
plt.show()

"""#### iii. Plot for Training No Bail Files in Frequency Proportion"""

tnb = X_train[X_train.index<=74]
tnb = np.sum(tnb, axis = 0)
tnb = tnb[tnb>3]
list1 = tnb[tnb != 0].index
# creating the Y Coordinate data as a python list
list2 = tnb[tnb != 0]
# plotting the bar graph
plt.barh(list1, list2/140)
# displaying the bar graph
plt.show()

"""### b. k-Nearest Neighbors Classifier"""

# Importing Jaccard Distance for our Bernoulli Data
from scipy.spatial.distance import jaccard
# Importing k-NN Classifier from Scikit Learn
from sklearn.neighbors import KNeighborsClassifier
# Splitting the Unbalanced Data into 80:20 proportion
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

"""#### i. Selecting suitable number of k"""

error_rate = []
for i in range(1,40):
 knn = KNeighborsClassifier(n_neighbors=i,metric = "jaccard")
 knn.fit(X_train,y_train)
 pred_i = knn.predict(X_test)
 error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed',marker='o',markerfacecolor='white', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
req_k_value = error_rate.index(min(error_rate))+1
print("Minimum error:-",min(error_rate),"at K =",req_k_value)

"""#### ii. Fitting k-NN"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 3, metric = 'jaccard')
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

"""#### iii.  Result of k-NN"""

# Importing Confusion Matrix from Sklearn
from sklearn.metrics import confusion_matrix,f1_score

cm1 = confusion_matrix(y_test,y_pred)
print('Confusion Matrix : \n', cm1)

total1=sum(sum(cm1))
#####from confusion matrix calculate accuracy
accuracy1=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Sensitivity : ', sensitivity1 )

specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

FScore= f1_score(y_test, y_pred)
print('F-Score : ', FScore)

# Confusion Matrix
conf_matrix = cm1
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.7)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

"""### c. Logistic Regression"""

X1 = X.copy()
#X1.drop(X.columns[X.sum(axis = 0) < 3] , axis = 1)

X_train, X_test, y_train, y_test= train_test_split(X1, y, test_size=0.2, random_state=0)

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()
# training the model
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.7)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

accuracy_score(y_test, y_pred)

import pandas as pd
X = pd.read_csv("IPC_Dataset.csv")

y = X.iloc[:,-1]
X = X.iloc[:,0:-1]

"""### d. Decision Tree"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=0)

"""#### i. Fitting Decision Tree"""

from sklearn import tree
tree_clf = tree.DecisionTreeClassifier(criterion = "entropy") #, max_depth = 5)
tree_clf = tree_clf.fit(X_train, y_train)

"""#### ii. Plotting the Decision Tree"""

#plot = tree.plot_tree(clf,figsize=(10,10))
#plot.figure(figsize=(10,10))
from matplotlib import pyplot as plt
fig,axes = plt.subplots(nrows = 1,ncols = 1,figsize = (5,5),dpi = 1200)
tree.plot_tree(tree_clf , filled = True)
[...]

y_pred = tree_clf.predict(X_test)
from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred)
ac = accuracy_score(y_test,y_pred)

cm

ac

"""#### iii. Cost Complexity Pruning for getting 'best' alpha value"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.tree import DecisionTreeClassifier
import sklearn.tree

path = tree_clf.cost_complexity_pruning_path(X_train , y_train)
alphas = path["ccp_alphas"]
#alphas

accuracy_train,accuracy_test = [],[]
for i in alphas :
    tree_clf = tree.DecisionTreeClassifier(criterion = "entropy",ccp_alpha = i)
    tree_clf.fit(X_train,y_train)
    y_test_pred = tree_clf.predict(X_test)
    y_train_pred = tree_clf.predict(X_train)
    accuracy_train.append(accuracy_score(y_train,y_train_pred))
    accuracy_test.append(accuracy_score(y_test,y_test_pred))

sns.set()
plt.figure(figsize = (14,7))
sns.lineplot(y=accuracy_train , x = alphas , label = "train_accuracy")
sns.lineplot(y=accuracy_test , x = alphas , label = "test_accuracy")
plt.xticks(ticks = np.arange(0,0.04,0.005))
plt.show()

"""#### iv. Fitting Decision Tree with the 'best' alpha value (tuning parameter)  """

t = tree.DecisionTreeClassifier(criterion = "entropy",ccp_alpha = 0.008)

"""#### v. Result for the Decision Tree with 'best' alpha value"""

t.fit(X_train,y_train)

y_pred = t.predict(X_test)
from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred)
ac = accuracy_score(y_test,y_pred)

cm

ac

conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.7)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

"""### e. Neural Network (NN)

#### i. NN with Scikit Learn
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

# Code for ANN

from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(hidden_layer_sizes=(74, 37, 18, 9, 4, 2), activation random_state=0)
# clf = MLPClassifier(hidden_layer_sizes=(64, 34, 4), random_state=0)
clf.fit(X_train, y_train)

#Let’s predict the test results
y_pred  =  clf.predict(X_test)

from sklearn.metrics import confusion_matrix,f1_score

# Confusion Matrix
cm1 = confusion_matrix(y_test,y_pred)
print('Confusion Matrix : \n', cm1)

# Getting sum of all cases (test)
total1=sum(sum(cm1))

# From confusion matrix calculate different confusin matrix metrics

accuracy1=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Sensitivity : ', sensitivity1 )

specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

FScore= f1_score(y_test, y_pred)
print('F-Score : ', FScore)

# Confusion Matrix
conf_matrix = cm1
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.7)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

"""#### ii. [NN with Keras to optimize the hidden layer](https://datagraphi.com/blog/post/2019/12/17/how-to-find-the-optimum-number-of-hidden-layers-and-nodes-in-a-neural-network-model)

###### 1. Importing the Libraries
"""

import pandas as pd
import math

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

"""###### 2. Optimization to get best hyperparameters for NN"""

# X= X.to_numpy()
# y = y.to_numpy()

# Function to linearly decrease or increase the number of nodes for the layers between the first layer and last layer

def FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes):
    layers = []

    nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)
    nodes = first_layer_nodes
    for i in range(1, n_layers+1):
        layers.append(math.ceil(nodes))
        nodes = nodes + nodes_increment

    return layers

# Function to vary the parameters of a tensor flow model by creating a new model based on given parameters

def createmodel(n_layers, first_layer_nodes, last_layer_nodes, activation_func, loss_func):
    model = Sequential()
    n_nodes = FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes)
    for i in range(1, n_layers):
        if i==1:
            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=activation_func))
        else:
            model.add(Dense(n_nodes[i-1], activation=activation_func))

    #Finally, the output layer should have a single node in binary classification
    model.add(Dense(1, activation=activation_func))
    model.compile(optimizer='adam', loss=loss_func, metrics = ["accuracy"]) #note: metrics could also be 'mse'

    return model

##Wrap model into scikit-learn
model =  KerasClassifier(build_fn=createmodel, verbose = False)

# Define the grid for searching the optimal parameters within the grid

activation_funcs = ['sigmoid', 'relu', 'tanh']
loss_funcs = ['binary_crossentropy','hinge']
param_grid = dict(n_layers=[2,3], first_layer_nodes = [64,32,16], last_layer_nodes = [4],  activation_func = activation_funcs, loss_func = loss_funcs, batch_size = [100], epochs = [20,60])
grid = GridSearchCV(estimator = model, param_grid = param_grid)

grid.fit(X,y)

print(grid.best_score_)
print(grid.best_params_)

pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']].to_csv('GridOptimization.csv')

# Getting the optimized Hidden layer
FindLayerNodesLinear(3, 64, 4)

"""###### 3. [Fitting the Model](https://www.section.io/engineering-education/build-ann-with-keras/#:~:text=%23%20Initializing%20the%20ANN%20ann%20%3D%20tf.keras.models.Sequential%20%28%29,Add%20the%20output%20layer%20ann.add%20%28tf.keras.layers.Dense%20%28units%3D1%2C%20activation%3D%27sigmoid%27%29%29)
The Values are changing with running everytime we have to set a seed.

###### I. With Keras suggested NN architecture
"""

## building Model Optimized Hidden layers

# split the dataset into train and test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

## Build the Artificial Neural Network
import tensorflow as tf
# Initializing the ANN
ann = tf.keras.models.Sequential()
# Add the input layer and first hidden layer
ann.add(tf.keras.layers.Dense(units= 64, activation='sigmoid', input_dim = X_train.shape[1]))
# Add the second hidden layer
ann.add(tf.keras.layers.Dense(units=34, activation='sigmoid'))
# Add the third layer
ann.add(tf.keras.layers.Dense(units=4, activation='sigmoid'))
# Add the output layer
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

## visualize the Artificial Neural Network
from tensorflow.keras.utils import plot_model
plot_model(ann,
           to_file="model.png",
           show_shapes=True,
           show_layer_names=True,
          )

## Training the ANN
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
ann.fit(X_train, y_train, batch_size = 20, epochs = 100)

y_pred = ann.predict(X_test)
y_pred = (y_pred > 0.5)

from sklearn.metrics import confusion_matrix, accuracy_score
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""###### II.  With manually gotten optimized architecture"""

## building Model Mechanically Optimized Hidden layers

# split the dataset into train and test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

## Build the Artificial Neural Network
import tensorflow as tf
# Initializing the ANN
ann = tf.keras.models.Sequential()
# Add the input layer and first hidden layer
ann.add(tf.keras.layers.Dense(units= 74, activation='relu', input_dim = X_train.shape[1]))
# Add the second hidden layer
ann.add(tf.keras.layers.Dense(units=37, activation='sigmoid'))
# Add the third layer
ann.add(tf.keras.layers.Dense(units=18, activation='sigmoid'))
# Add the fourth layer
ann.add(tf.keras.layers.Dense(units=9, activation='sigmoid'))
# Add the fifth layer
ann.add(tf.keras.layers.Dense(units=4, activation='sigmoid'))
# Add the sixth layer
ann.add(tf.keras.layers.Dense(units=2, activation='sigmoid'))
# Add the output layer
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

## (74, 37, 18, 9, 4, 2)
## visualize the Artificial Neural Network
from tensorflow.keras.utils import plot_model
plot_model(ann,
           to_file="model.png",
           show_shapes=True,
           show_layer_names=True,
          )

## Training the ANN
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
ann.fit(X_train, y_train, batch_size = 20, epochs = 100)

y_pred = ann.predict(X_test)
y_pred = (y_pred > 0.5)

from sklearn.metrics import confusion_matrix, accuracy_score
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""###### III. To visualize the NN Architechture"""

# ! pip install ann_visualizer

from ann_visualizer.visualize import ann_viz;
ann_viz(ann, title ="");

"""### Decision Tree for unbalanced data"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X_balanced.iloc[:,0:-1], X_balanced.iloc[:,-1], test_size=0.2, random_state=78)

from sklearn import tree
tree_clf = tree.DecisionTreeClassifier(criterion = "entropy" ,ccp_alpha = 0.005)
tree_clf = tree_clf.fit(X_train, y_train)

#plot = tree.plot_tree(clf,figsize=(10,10))
#plot.figure(figsize=(10,10))
#from matplotlib import pyplot as plt
#fig,axes = plt.subplots(nrows = 1,ncols = 1,figsize = (5,5),dpi = 1200)
#tree.plot_tree(tree_clf , filled = True)
#[...]



y_pred = tree_clf.predict(X_test)
from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred)
ac = accuracy_score(y_test,y_pred)

cm

ac

cm

### Preparing Data for the Resampling (concating X_train and y_train before running resampling)

# X_train_bs = pd.concat([X_train, y_train], axis = 1)
# X_train_nb = X_train_bs.loc[X_train_bs.index<170]
# X_train_b = X_train_bs.loc[X_train_bs.index>=170]
# X_train_b = X_train.loc[X_train.index>=170]
# X_test_bs = pd.concat([X_test, y_test], axis = 1)
# X_test_nb = X_test_bs.loc[X_test_bs.index<170]

# X_test_nb

#X_train.loc[X_train.index>=170]

# Resampling of all the data at the starting

# BootStrap_Sample_ = df.sample(345, replace = True, random_state = 9)

# BootStrap_Sample
# X_balanced = pd.concat([BootStrap_Sample , df_ipc_zo.iloc[170:, :]],axis = 0)

#X_balanced

"""### F. Resampling:

#### a. ***Over Sampling:***
"""

### Preparing Data for the Resampling (concating X_train and y_train before running resampling)

X_train_bs = pd.concat([X_train, y_train], axis = 1)
X_train_nb = X_train_bs.loc[X_train_bs.index<170]
# X_train_b = X_train_bs.loc[X_train_bs.index>=170]
# X_train_nb

# Resampling of X_train,

# Resampling for the X_train
X_train_bs_nb = X_train_nb.sample(271, replace = True, random_state = 9)

# Concating the the resampled nb data with b data
a = pd.concat([X_train.loc[X_train.index>=170] , y_train.loc[X_train.index>=170]], axis=1)
X_train_bs_f = pd.concat([X_train_bs_nb, a], axis=0)

# Taking all the columns leaving label column
X_train_bs   = X_train_bs_f.iloc[:, 0:-1]

# Taking only the label column
y_train_bs   = X_train_bs_f.iloc[:,-1]

"""##### i. Classification with Over Sampling

###### 1. k-NN Classifier
"""

error_rate = []
for i in range(1,40):
 knn = KNeighborsClassifier(n_neighbors=i,metric = "jaccard")
 knn.fit(X_train_bs,y_train_bs)
 pred_i = knn.predict(X_test_bs)
 error_rate.append(np.mean(pred_i != y_test_bs))

plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed',marker='o',markerfacecolor='white', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
req_k_value = error_rate.index(min(error_rate))+1
print("Minimum error:-",min(error_rate),"at K =",req_k_value)

# K-NN OVERSAMPLED
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 24, metric = 'jaccard')
classifier.fit(X_train_bs, y_train_bs)

from sklearn.metrics import confusion_matrix,f1_score

y_pred = classifier.predict(X_test)
cm1 = confusion_matrix(y_test,y_pred)
print('Confusion Matrix : \n', cm1)

total1=sum(sum(cm1))
#####from confusion matrix calculate accuracy
accuracy1=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Sensitivity : ', sensitivity1 )

specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

FScore= f1_score(y_test, y_pred)
print('F-Score : ', FScore)

#np.sum((X_train_bs.index>=170))

"""###### 2. Bernoulli Naive Bayes"""

# Naive Bayes with oversampeled
from sklearn.naive_bayes import BernoulliNB
classifier = BernoulliNB()
classifier.fit(X_train_bs, y_train_bs)

#Let’s predict the test results
y_pred  =  classifier.predict(X_test)

from sklearn.metrics import confusion_matrix,f1_score

cm1 = confusion_matrix(y_test,y_pred)
print('Confusion Matrix : \n', cm1)

total1=sum(sum(cm1))
#####from confusion matrix calculate accuracy
accuracy1=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Sensitivity : ', sensitivity1 )

specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

FScore= f1_score(y_test, y_pred)
print('F-Score : ', FScore)

"""###### 3. Neural network with Over Sampling:"""

# # Concating the the resampled nb data with b data
# a = pd.concat([X_train.loc[X_train.index>=170] , y_train.loc[X_train.index>=170]], axis=1) # Bail Files
# X_train_bs_f = pd.concat([X_train_bs_nb, a], axis=0)

# # Taking all the columns leaving label column
# X_train_bs   = X_train_bs_f.iloc[:, 0:-1]
# # Taking only the label column
# y_train_bs   = X_train_bs_f.iloc[:,-1]

# X_overSampled = pd.concat([X_train_bs, X_test])
# y_overSampled = pd.concat([y_train_bs, y_test])
# X_overSampled ## The Size coming according to: Bail_train = 271, No_Bail_train = 271, Bail_test = 73, No_Bail_test = 30, sum= 645.

import pandas as pd
import math

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# X= X.to_numpy()
# y = y.to_numpy()

# Function to linearly decrease or increase the number of nodes for the layers between the first layer and last layer

def FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes):
    layers = []

    nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)
    nodes = first_layer_nodes
    for i in range(1, n_layers+1):
        layers.append(math.ceil(nodes))
        nodes = nodes + nodes_increment

    return layers

# Function to vary the parameters of a tensor flow model by creating a new model based on given parameters

def createmodel(n_layers, first_layer_nodes, last_layer_nodes, activation_func, loss_func):
    model = Sequential()
    n_nodes = FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes)
    for i in range(1, n_layers):
        if i==1:
            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=activation_func))
        else:
            model.add(Dense(n_nodes[i-1], activation=activation_func))

    #Finally, the output layer should have a single node in binary classification
    model.add(Dense(1, activation=activation_func))
    model.compile(optimizer='adam', loss=loss_func, metrics = ["accuracy"]) #note: metrics could also be 'mse'

    return model

##Wrap model into scikit-learn
model =  KerasClassifier(build_fn=createmodel, verbose = False)

# Define the grid for searching the optimal parameters within the grid

activation_funcs = ['sigmoid', 'relu', 'tanh']
loss_funcs = ['binary_crossentropy','hinge']
param_grid = dict(n_layers=[2,3], first_layer_nodes = [64,32,16], last_layer_nodes = [4],  activation_func = activation_funcs, loss_func = loss_funcs, batch_size = [100], epochs = [20,60])
grid = GridSearchCV(estimator = model, param_grid = param_grid)

grid.fit(X_overSampled,y_overSampled)

print(grid.best_score_)
print(grid.best_params_)

pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']].to_csv('GridOptimizationOverSampled.csv')

FindLayerNodesLinear(3, 32, 4)

## building Model Optimized Hidden layers

# split the dataset into train and test set

## Build the Artificial Neural Network
import tensorflow as tf
# Initializing the ANN
ann = tf.keras.models.Sequential()
# Add the input layer and first hidden layer
ann.add(tf.keras.layers.Dense(units= 32, activation='sigmoid', input_dim = X_train_bs.shape[1]))
# Add the second hidden layer
ann.add(tf.keras.layers.Dense(units= 18, activation='sigmoid'))
# Add the third layer
ann.add(tf.keras.layers.Dense(units= 4, activation='sigmoid'))
# Add the output layer
ann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))

## visualize the Artificial Neural Network
from tensorflow.keras.utils import plot_model
plot_model(ann,
           to_file="model.png",
           show_shapes=True,
           show_layer_names=True,
          )

## Training the ANN
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
ann.fit(X_train_bs, y_train_bs, batch_size = 20, epochs = 100)

y_pred = ann.predict(X_test)
y_pred = (y_pred > 0.5)

from sklearn.metrics import confusion_matrix, accuracy_score
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""#### b. ***Under Sampling:***"""

## Bail Files (X & y)
X_b = df.iloc[170:, :]
X_b
## No Bail Files (X & y)
X_nb = df.iloc[:170, :]

## Selection of 170 files out of 344 Bail Files
X_bs_b = X_b.sample(170, replace = True, random_state = 9)

# Joining 170 randomly chosen files Bail file with other 170 No Bails
X_bs_us = pd.concat([X_nb, X_bs_b], axis=0)
#X_bs_us

"""##### i. Classification with Under Sampling

###### 1. k-NN Classifier
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X_bs_us.iloc[:,0:-1], X_bs_us.iloc[:,-1], test_size=0.2, random_state=78)

error_rate = []
for i in range(1,40):
 knn = KNeighborsClassifier(n_neighbors=i,metric = "jaccard")
 knn.fit(X_train,y_train)
 pred_i = knn.predict(X_test)
 error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed',marker='o',markerfacecolor='white', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
req_k_value = error_rate.index(min(error_rate))+1
print("Minimum error:-",min(error_rate),"at K =",req_k_value)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 3, metric = 'jaccard')
classifier.fit(X_train, y_train)


from sklearn.metrics import confusion_matrix,f1_score

y_pred = classifier.predict(X_test)
cm1 = confusion_matrix(y_test,y_pred)
print('Confusion Matrix : \n', cm1)

total1=sum(sum(cm1))
#####from confusion matrix calculate accuracy
accuracy1=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Sensitivity : ', sensitivity1 )

specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

FScore= f1_score(y_test, y_pred)
print('F-Score : ', FScore)

"""###### 2. Bernoulli Naive Bayes"""

#from sklearn.model_selection import train_test_split
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

#Training the Naive Bayes model on the training set

from sklearn.naive_bayes import BernoulliNB
classifier = BernoulliNB()
classifier.fit(X_train, y_train)

#Let’s predict the test results
y_pred  =  classifier.predict(X_test)

from sklearn.metrics import confusion_matrix,f1_score

# Confusion Matrix
cm1 = confusion_matrix(y_test,y_pred)
print('Confusion Matrix : \n', cm1)

# Getting sum of all cases (test)
total1=sum(sum(cm1))

# From confusion matrix calculate different confusin matrix metrics

accuracy1=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Sensitivity : ', sensitivity1 )

specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

FScore= f1_score(y_test, y_pred)
print('F-Score : ', FScore)

"""### Over Sampling and ANN

## Creation of Over Samples
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
X_train

# Resampling of X_train,

# Resampling for the X_train
X_train_bs_nb = X_train_nb.sample(271, replace = True, random_state = 9)

"""## Random Forest(Bagging)

# New Section
"""

# evaluate bagging algorithm for classification
from numpy import mean
from numpy import std
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import BaggingClassifier

model = BaggingClassifier()
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

# split the dataset into train and test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

model.fit(X_train, y_train)

#Let’s predict the test results
y_pred  =  model.predict(X_test)

from sklearn.metrics import confusion_matrix,f1_score

# Confusion Matrix
cm1 = confusion_matrix(y_test,y_pred)
print('Confusion Matrix : \n', cm1)

# Getting sum of all cases (test)
total1=sum(sum(cm1))

# From confusion matrix calculate different confusin matrix metrics

accuracy1=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Sensitivity : ', sensitivity1 )

specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

FScore= f1_score(y_test, y_pred)
print('F-Score : ', FScore)

